{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Haiku GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PerceptronV/Apollo-Psi/blob/master/Haiku_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rFpeC773L3b",
        "colab_type": "text"
      },
      "source": [
        "#*Haiku GAN*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxdtCMdvAQL9",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyJ0uoTokx_i",
        "colab_type": "text"
      },
      "source": [
        "## Importing and Downloading Relevant Libraries; Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hySK3S4uAjmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install vibra --upgrade"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpps66mCWi8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print ('Apollo Î³ ------ initiating')\n",
        "\n",
        "# Imp.d numpy [as np]; os; time; tensorflow [as tf]; future [absolute_import,division,print_function,unicode_literals]\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow import keras\n",
        "import vibra\n",
        "from vibra import firefly as ff\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUv0AeigWxb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = ff.file(filename='haikus.txt', src='https://raw.githubusercontent.com/PerceptronV/Apollo-sources/master/{Haikus}.txt')\n",
        "\n",
        "\n",
        "char2idx = file.char2idx()\n",
        "idx2char = file.idx2char()\n",
        "vocab_size = file.char_size()\n",
        "print (char2idx)\n",
        "print (str(vocab_size))\n",
        "print (char2idx['}'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4WCP_V6qPfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text=file.text()\n",
        "text_as_list=text.split(\"}\\n\\n\")\n",
        "text_as_int=[]\n",
        "for seg in text_as_list:\n",
        "  text_as_int.append([char2idx[c] for c in seg])\n",
        "\n",
        "max_length=0  \n",
        "\n",
        "for asdf in text_as_int:\n",
        "  if len(asdf) > max_length:\n",
        "    max_length=len(asdf)\n",
        "\n",
        "for lkjh in range(len(text_as_int)):\n",
        "  #padding\n",
        "  if len(text_as_int[lkjh]) < max_length:\n",
        "    for qwer in range (max_length - len(text_as_int[lkjh])):\n",
        "      text_as_int[lkjh].append(0)\n",
        "  text_as_int[lkjh]=[text_as_int[lkjh]]\n",
        "\n",
        "dataset=np.asarray(text_as_int)\n",
        "print (dataset[0])\n",
        "print (dataset.shape)\n",
        "\n",
        "    \n",
        "print (max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OqxQk1mlP0B",
        "colab_type": "text"
      },
      "source": [
        "## Making the Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNzgpY0KlvjX",
        "colab_type": "text"
      },
      "source": [
        "### Making the seed generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LyaNrvWlYD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_seed(vocab_size):\n",
        "  \n",
        "  seed=np.random.randint(1, size=(1,vocab_size))\n",
        "      \n",
        "  return (seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVOIL47al1Mo",
        "colab_type": "text"
      },
      "source": [
        "###Defining the Generator model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I6_K4_OWs1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_generator (vocab_size,max_length):\n",
        "  \n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32,input_shape=(vocab_size,)),\n",
        "    keras.layers.Activation('sigmoid'),\n",
        "    tf.keras.layers.Dense(1024),\n",
        "    keras.layers.Activation('exponential'),\n",
        "    tf.keras.layers.Dense(1024),\n",
        "    tf.keras.layers.Dense(max_length),\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "'''def make_generator (vocab_size,max_length):\n",
        "  \n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 256,\n",
        "                              batch_input_shape=[1, None]),\n",
        "    rnn(1024,\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=False),\n",
        "    tf.keras.layers.Dense(1024),\n",
        "    tf.keras.layers.Dense(max_length)\n",
        "  ])\n",
        "  return model'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trq9aaWza9Tk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Define text generation process\n",
        "def gen_text (model,seed=None,training=False):\n",
        "  \n",
        "  if seed is None:\n",
        "    seed=gen_seed()\n",
        "    \n",
        "  raw=[]\n",
        "\n",
        "  temperature = 0.5\n",
        "\n",
        "  model.reset_states()\n",
        "  \n",
        "  input_eval=seed\n",
        "  \n",
        "  while (1):\n",
        "      predictions = model(input_eval,training=training)      \n",
        "      ''''''\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      \n",
        "      #print (predictions.shape)\n",
        "      \n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "      place=idx2char[predicted_id]\n",
        "      \n",
        "      if place == '}':\n",
        "        break\n",
        "        \n",
        "      raw.append(predicted_id)'''\n",
        "      \n",
        "      '''print (predictions.shape)\n",
        "      predicted_id=np.argmax(predictions)'''\n",
        "      \n",
        "      \n",
        "      \n",
        "\n",
        "  \n",
        "  '''raw=np.asarray(raw)    \n",
        "  return (raw)'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghVSATmlmFi3",
        "colab_type": "text"
      },
      "source": [
        "###Testing the untrained Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gGIY2SpGv_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = make_generator(vocab_size,max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_nOfsIOZTP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txt=generator(gen_seed(vocab_size))\n",
        "\n",
        "print (txt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tgI2xIomVrR",
        "colab_type": "text"
      },
      "source": [
        "##Making the Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsj-C0Qzmapb",
        "colab_type": "text"
      },
      "source": [
        "###Defining the Discriminator model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hLwAsJsmjaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_discriminator (max_length):\n",
        "  \n",
        "  \n",
        "  model = tf.keras.Sequential([\n",
        "      \n",
        "    tf.keras.layers.Dense(56,input_shape=(max_length,)),\n",
        "    tf.keras.layers.Dense(1024),\n",
        "    tf.keras.layers.Dense(1),\n",
        "    tf.keras.layers.Activation('sigmoid')])\n",
        "  return model\n",
        "\n",
        "'''def make_discriminator (vocab_size):\n",
        "  \n",
        "  \n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 256),\n",
        "    rnn(1024,\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',),\n",
        "      \n",
        "    tf.keras.layers.Dense(1024),\n",
        "    tf.keras.layers.Dense(1),\n",
        "    tf.keras.layers.Activation('sigmoid')])\n",
        "  return model'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xABU8OcegsId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''def dis_text (model,inp,training=False):\n",
        "  \n",
        "\n",
        "  inp=tf.expand_dims(inp,0)\n",
        "  model.reset_states()\n",
        "  \n",
        "  dis = model(inp,training=training)\n",
        "  dis=tf.squeeze(dis,[0,2])\n",
        "  out=dis[len(dis)-1]\n",
        "  out=tf.reshape(out,[1,1])\n",
        "  \n",
        "  return (out)'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quSrArEJmtAN",
        "colab_type": "text"
      },
      "source": [
        "###Testing the untrained Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPOgkPOCpOgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator = make_discriminator(max_length)\n",
        "a = discriminator(txt)\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lTF1HnfpNIo",
        "colab_type": "text"
      },
      "source": [
        "##Defining the losses "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoX7ph0cpkhS",
        "colab_type": "text"
      },
      "source": [
        "###Global loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvUUVaUqukta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5c4wRyFprUV",
        "colab_type": "text"
      },
      "source": [
        "###Discriminator loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXp5Q-HTwCIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO4h381ZCxya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (discriminator_loss(discriminator(dataset[0]),a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a71Cta7Ypuee",
        "colab_type": "text"
      },
      "source": [
        "###Generator loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eael9WcFwJ_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DRY8pnB8XLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generator_loss(a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XncLY6gPBSlw",
        "colab_type": "text"
      },
      "source": [
        "##Defining the training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZmBBb8Gq1hl",
        "colab_type": "text"
      },
      "source": [
        "###Defining the optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-6K2ksGqzAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_optimizer = tf.train.AdamOptimizer()\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbtpGc82B109",
        "colab_type": "text"
      },
      "source": [
        "###Saving Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsndnR2LBYv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXrKsHlLDBTU",
        "colab_type": "text"
      },
      "source": [
        "###Initializing the training process variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RSgdQi4DOSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50\n",
        "seed = gen_seed(vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl6h7riMvKXB",
        "colab_type": "text"
      },
      "source": [
        "###Defining one training step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3ZYhcKrDIee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp):\n",
        "     \n",
        "    for tyui in range (10):\n",
        "      \n",
        "      with tf.GradientTape() as disc_tape:\n",
        "      \n",
        "        generated_text = generator(gen_seed(vocab_size))\n",
        "\n",
        "        real_output = discriminator(inp)\n",
        "        fake_output = discriminator(generated_text)\n",
        "        \n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "        \n",
        "      gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    \n",
        "    for tyui in range (10):\n",
        "      \n",
        "      with tf.GradientTape() as gen_tape:\n",
        "\n",
        "        generated_text = generator(gen_seed(vocab_size))\n",
        "        \n",
        "        fake_output = discriminator(generated_text)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "\n",
        "      gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "      generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "    return gen_loss,disc_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swIWJwnavOvn",
        "colab_type": "text"
      },
      "source": [
        "###Defining the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbVbr2VVtPcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(vocab_size,dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    start = time.time()\n",
        "    \n",
        "    for i in dataset:\n",
        "      g,d=train_step(i)\n",
        "    \n",
        "    print (\"EPOCH {}:\".format(epoch + 1))\n",
        "    \n",
        "    out=generator(seed)\n",
        "    print (out)\n",
        "\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec\\nGenerator loss: {}\\nDiscriminator loss: {}'\\\n",
        "           .format(epoch + 1, time.time()-start,g,d))\n",
        "    \n",
        "    out=generator(seed)\n",
        "  \n",
        "    fout=''\n",
        "\n",
        "    for i2 in out[0]:\n",
        "      if round(int(i2))<vocab_size and round (int(i2)) > 0:\n",
        "        fout+=str(idx2char[round(int(i2))])\n",
        "\n",
        "    print ('\\n'+fout+'\\n\\n\\n')\n",
        "\n",
        "  \n",
        "  print (\"\\nAfter {} epochs:\".format(epochs))\n",
        "  out=generator(seed)\n",
        "  \n",
        "  fout=''\n",
        "\n",
        "  for i2 in out[0]:\n",
        "    if round(int(i2))<vocab_size and round (int(i2)) > 0:\n",
        "      fout+=str(idx2char[round(int(i2))])\n",
        "  print ('\\n'+fout+'\\n\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJaKkr5qyM0I",
        "colab_type": "text"
      },
      "source": [
        "###Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9T00GwWyRzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "train(vocab_size, dataset, EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ikv-CqVlp1nX",
        "colab_type": "text"
      },
      "source": [
        "##Main Source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ1dfxeQuVU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print ('Apollo Î³ ------ initiating\\n\\n')\n",
        "\n",
        "!pip install vibra --upgrade\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow import keras\n",
        "import vibra\n",
        "from vibra import firefly as ff\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "def gen_seed(vocab_size):\n",
        "  \n",
        "  seed=np.random.randint(1, size=(1,vocab_size))\n",
        "      \n",
        "  return (seed)\n",
        "\n",
        "def make_generator (vocab_size,max_length):\n",
        "  \n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32,input_shape=(vocab_size,)),\n",
        "    keras.layers.Activation('sigmoid'),\n",
        "    tf.keras.layers.Dense(1024),\n",
        "    keras.layers.Activation('exponential'),\n",
        "    tf.keras.layers.Dense(1024),\n",
        "    tf.keras.layers.Dense(max_length),\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def make_discriminator (max_length):\n",
        "  \n",
        "  \n",
        "  model = tf.keras.Sequential([\n",
        "      \n",
        "    tf.keras.layers.Dense(56,input_shape=(max_length,)),\n",
        "    tf.keras.layers.Dense(1024),\n",
        "    tf.keras.layers.Dense(1),\n",
        "    tf.keras.layers.Activation('sigmoid')])\n",
        "  return model\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "generator_optimizer = tf.train.AdamOptimizer()\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "  \n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "  \n",
        "@tf.function\n",
        "def train_step(inp):\n",
        "     \n",
        "    for tyui in range (10):\n",
        "      \n",
        "      with tf.GradientTape() as disc_tape:\n",
        "      \n",
        "        generated_text = generator(gen_seed(vocab_size))\n",
        "\n",
        "        real_output = discriminator(inp)\n",
        "        fake_output = discriminator(generated_text)\n",
        "        \n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "        \n",
        "      gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    \n",
        "    for tyui in range (10):\n",
        "      \n",
        "      with tf.GradientTape() as gen_tape:\n",
        "\n",
        "        generated_text = generator(gen_seed(vocab_size))\n",
        "        \n",
        "        fake_output = discriminator(generated_text)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "\n",
        "      gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "      generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "    return gen_loss,disc_loss\n",
        "  \n",
        "def train(vocab_size,dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    start = time.time()\n",
        "    \n",
        "    for i in dataset:\n",
        "      g,d=train_step(i)\n",
        "    \n",
        "    print (\"EPOCH {}:\".format(epoch + 1))\n",
        "    \n",
        "    out=generator(seed)\n",
        "    print (out)\n",
        "\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec\\nGenerator loss: {}\\nDiscriminator loss: {}'\\\n",
        "           .format(epoch + 1, time.time()-start,g,d))\n",
        "    \n",
        "    out=generator(seed)\n",
        "  \n",
        "    fout=''\n",
        "\n",
        "    for i2 in out[0]:\n",
        "      if round(int(i2))<vocab_size and round (int(i2)) > 0:\n",
        "        fout+=str(idx2char[round(int(i2))])\n",
        "\n",
        "    print ('\\n'+fout+'\\n\\n\\n')\n",
        "\n",
        "  \n",
        "  print (\"\\nAfter {} epochs:\".format(epochs))\n",
        "  out=generator(seed)\n",
        "  \n",
        "  fout=''\n",
        "\n",
        "  for i2 in out[0]:\n",
        "    if round(int(i2))<vocab_size and round (int(i2)) > 0:\n",
        "      fout+=str(idx2char[round(int(i2))])\n",
        "  print ('\\n'+fout+'\\n\\n\\n')\n",
        "\n",
        "file = ff.file(filename='haikus.txt', src='https://raw.githubusercontent.com/PerceptronV/Apollo-sources/master/{Haikus}.txt')\n",
        "\n",
        "char2idx = file.char2idx()\n",
        "idx2char = file.idx2char()\n",
        "vocab_size = file.char_size()\n",
        "\n",
        "text=file.text()\n",
        "text_as_list=text.split(\"}\\n\\n\")\n",
        "text_as_int=[]\n",
        "\n",
        "for seg in text_as_list:\n",
        "  text_as_int.append([char2idx[c] for c in seg])\n",
        "\n",
        "max_length=0  \n",
        "\n",
        "for asdf in text_as_int:\n",
        "  if len(asdf) > max_length:\n",
        "    max_length=len(asdf)\n",
        "\n",
        "for lkjh in range(len(text_as_int)):\n",
        "  if len(text_as_int[lkjh]) < max_length:\n",
        "    for qwer in range (max_length - len(text_as_int[lkjh])):\n",
        "      text_as_int[lkjh].append(0)\n",
        "  text_as_int[lkjh]=[text_as_int[lkjh]]\n",
        "\n",
        "dataset=np.asarray(text_as_int)\n",
        "\n",
        "generator = make_generator(vocab_size,max_length)\n",
        "\n",
        "discriminator = make_discriminator(max_length)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "EPOCHS = 50\n",
        "seed = gen_seed(vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA7-WeXxdb-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "train(vocab_size, dataset, EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}